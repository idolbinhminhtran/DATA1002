{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f5b372-031e-4d95-9060-edc19f4d617e",
   "metadata": {},
   "source": [
    "---\n",
    "# __Logistic Regression with Trailing 5-Match xG Averages and Time-Based Splits__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c167e-3ef3-4aba-9774-186436e6a326",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "## 1. Load the Dataset and Parse Match Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6accadd5-3786-4487-bafc-d287864f681a",
   "metadata": {},
   "source": [
    "- First, we load the dataset and convert the 'Date' column to datetime format. This ensures we can easily filter matches by date for chronological splitting. We use pandas' to_datetime for the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b93019-1dec-4bd3-9722-64ed0d6093d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Premier League dataset\n",
    "df = pd.read_csv('PL_integrated_dataset_10years.csv')\n",
    "\n",
    "# Convert the 'Date' column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Verify conversion (the 'Date' column should now have a datetime dtype)\n",
    "print(df['Date'].dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103506dd-d3f4-4fb3-b4bd-294a909bbbee",
   "metadata": {},
   "source": [
    "- __Explanation__: After this step, the Date column is of type datetime (datetime64[ns]), which will allow us to perform time-based filtering and extract components like year or month easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfc769-6d66-4450-b8ab-2af96bf84e0c",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Extract Season/Year Information from Dates\n",
    "---\n",
    "\n",
    "Next, we extract useful time information from the date. Specifically, we'll derive the year and (if needed) the season of each match. The Premier League season typically runs from August to May, crossing calendar years. We'll determine the \"season year\" for each match by using the match date:\n",
    "\n",
    "If a match is in July or later (i.e., the second half of the calendar year), we'll treat it as part of the season that starts that year.\n",
    "\n",
    "If a match is in January through June, it belongs to the season that started the previous year.\n",
    "\n",
    "Let's create a new column to identify the season by its starting year, and also extract the calendar year and month for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e56711e-8f59-4c33-a943-f6769a710c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date Season_Label\n",
      "0 2015-08-08    2015-2016\n",
      "1 2015-08-08    2015-2016\n",
      "2 2015-08-08    2015-2016\n"
     ]
    }
   ],
   "source": [
    "# Extract year and month from the date\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "# Determine the season's start year for each match\n",
    "df['SeasonStartYear'] = df['Date'].apply(lambda x: x.year if x.month >= 7 else x.year - 1)\n",
    "\n",
    "# (Optional) Combine to a season label, e.g., \"2023-2024\"\n",
    "df['Season_Label'] = df['SeasonStartYear'].astype(str) + '-' + (df['SeasonStartYear'] + 1).astype(str)\n",
    "print(df[['Date', 'Season_Label']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50390f-434a-4c8f-b83e-41e66476d31f",
   "metadata": {},
   "source": [
    "__Explanation__: We added:\n",
    "\n",
    "- __\"Year\"__ and __\"Month\"__ columns for the calendar year and month of each match.\n",
    "\n",
    "- __\"SeasonStartYear\"__ which uses the rule above (e.g., a match on 2023-08-15 gets SeasonStartYear=2023, while a match on 2024-05-10 gets SeasonStartYear=2023 because it's before July 2024).\n",
    "\n",
    "- __\"Season_Label\"__ combines the start year and the next year (for instance, a match in August 2023 gets label \"2023-2024\").\n",
    "\n",
    "-> This confirms which season each match belongs to, which will help ensure our splits align with season boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1591a-dab3-4f33-9cd5-c60a21412469",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Chronological Train, Validation, and Test Splits\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ea901-2d55-48c0-bddf-4f6e1871c3be",
   "metadata": {},
   "source": [
    "Now we split the dataset into three sets based on date following the __8/1/1__ principle where 8 seasons are for training the model, 1 season for  validation and 1 for testing the final performance.\n",
    "\n",
    "- __Training set__: All matches before July 1, 2023 (i.e., up to June 30, 2023).\n",
    "\n",
    "- __Validation set__: Matches from July 1, 2023 through June 30, 2024 (the 2023–2024 season).\n",
    "\n",
    "- __Test set__: Matches from July 1, 2024 onward (the 2024–2025 season).\n",
    "\n",
    "We create boolean masks for these date ranges and then apply them to the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1ca237-8a49-40c5-a876-3f1c65ef05f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set matches: 3014\n",
      "Validation set matches: 380\n",
      "Test set matches: 380\n"
     ]
    }
   ],
   "source": [
    "# Define cutoff dates for splitting\n",
    "train_cutoff = pd.Timestamp('2023-07-01')   # start of validation period\n",
    "test_cutoff  = pd.Timestamp('2024-07-01')   # start of test period (2024-2025 season)\n",
    "\n",
    "# Create boolean masks for each subset\n",
    "train_mask = df['Date'] < train_cutoff\n",
    "val_mask   = (df['Date'] >= train_cutoff) & (df['Date'] < test_cutoff)\n",
    "test_mask  = df['Date'] >= test_cutoff\n",
    "\n",
    "# Apply masks to create the splits\n",
    "train_df = df[train_mask].copy()\n",
    "val_df   = df[val_mask].copy()\n",
    "test_df  = df[test_mask].copy()\n",
    "\n",
    "# Check the number of matches in each subset\n",
    "print(\"Training set matches:\", len(train_df))\n",
    "print(\"Validation set matches:\", len(val_df))\n",
    "print(\"Test set matches:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ec4d3-f817-4341-9241-a1b82ba7655b",
   "metadata": {},
   "source": [
    "__Explanation__: We use July 1, 2023 as the boundary between training and validation, and July 1, 2024 as the boundary between validation and test. The .copy() is used to create independent DataFrames for each subset. The printed counts show how many matches fall in each set (the training set should comprise matches from 2015 up to mid-2023, and validation/test sets each roughly cover one season of 380 matches). At this stage, these splits include all matches in the date ranges, including some early-season matches that may not have trailing 5-match averages computed yet. We will handle those next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d98486-fdee-4b45-bb42-e3769afbcae2",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Compute Trailing 5-Match Average xG Features and Filter Invalid Rows\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff64faf-0b62-4e95-abf6-5c7e22cd46c6",
   "metadata": {},
   "source": [
    "We now create the features for the trailing 5-match average expected goals (xG) for both home and away teams. This feature represents each team's average xG over its last 5 matches prior to the current match.\n",
    "\n",
    "Steps to compute trailing averages:\n",
    "\n",
    "__1)__ Combine the home and away xG data into a single timeline for each team.\n",
    "\n",
    "__2)__ Sort matches for each team by date.\n",
    "\n",
    "__3)__ Compute a rolling mean of xG over the last 5 games for each team, shifting by one to ensure it's strictly the previous 5 matches (the current match's xG is not included in its own average).\n",
    "\n",
    "__4)__ Assign these averages back to the main dataset in two new columns: home_team_avg_xG_last5 and away_team_avg_xG_last5.\n",
    "\n",
    "__5)__ Some matches (typically at the start of a season or when a team is newly promoted) will not have 5 prior games to compute an average, resulting in NaN for those rows. We will drop those rows, as instructed, so that both home and away trailing xG values are valid.\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd79bdb-f9b3-4100-8a9f-452478f4e93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (after drop) matches: 2905\n",
      "Validation set (after drop) matches: 375\n",
      "Test set (after drop) matches: 375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize new columns in the original DataFrame for trailing xG averages\n",
    "df['home_team_avg_xG_last5'] = np.nan\n",
    "df['away_team_avg_xG_last5'] = np.nan\n",
    "\n",
    "# Prepare a combined dataset of team performances to compute rolling averages\n",
    "home_stats = pd.DataFrame({\n",
    "    'Team': df['HomeTeam'],\n",
    "    'Date': df['Date'],\n",
    "    'xG': df['Home_xG'],          # home team's xG in that match\n",
    "    'MatchID': df.index,\n",
    "    'Role': 'home'\n",
    "})\n",
    "away_stats = pd.DataFrame({\n",
    "    'Team': df['AwayTeam'],\n",
    "    'Date': df['Date'],\n",
    "    'xG': df['Away_xG'],          # away team's xG in that match\n",
    "    'MatchID': df.index,\n",
    "    'Role': 'away'\n",
    "})\n",
    "combined_stats = pd.concat([home_stats, away_stats], ignore_index=True)\n",
    "\n",
    "# Sort by team and date to ensure chronological order within each team\n",
    "combined_stats.sort_values(['Team', 'Date'], inplace=True)\n",
    "\n",
    "# Group by team and compute rolling mean of the last 5 xG values (excluding current match by shifting)\n",
    "combined_stats['xG_avg_last5'] = combined_stats.groupby('Team')['xG']\\\n",
    "    .transform(lambda x: x.rolling(window=5, min_periods=5).mean().shift(1))\n",
    "\n",
    "# Assign the computed averages back to the original DataFrame for each match\n",
    "# We use the stored MatchID and Role to place the value in the correct home/away column\n",
    "home_mask = combined_stats['Role'] == 'home'\n",
    "away_mask = combined_stats['Role'] == 'away'\n",
    "\n",
    "# For home team entries, assign to home_team_avg_xG_last5\n",
    "df.loc[combined_stats.loc[home_mask, 'MatchID'], 'home_team_avg_xG_last5'] = combined_stats.loc[home_mask, 'xG_avg_last5'].values\n",
    "# For away team entries, assign to away_team_avg_xG_last5\n",
    "df.loc[combined_stats.loc[away_mask, 'MatchID'], 'away_team_avg_xG_last5'] = combined_stats.loc[away_mask, 'xG_avg_last5'].values\n",
    "\n",
    "# Now update the splits DataFrames with these new features\n",
    "train_df['home_team_avg_xG_last5'] = df.loc[train_df.index, 'home_team_avg_xG_last5'].values\n",
    "train_df['away_team_avg_xG_last5'] = df.loc[train_df.index, 'away_team_avg_xG_last5'].values\n",
    "\n",
    "val_df['home_team_avg_xG_last5'] = df.loc[val_df.index, 'home_team_avg_xG_last5'].values\n",
    "val_df['away_team_avg_xG_last5'] = df.loc[val_df.index, 'away_team_avg_xG_last5'].values\n",
    "\n",
    "test_df['home_team_avg_xG_last5'] = df.loc[test_df.index, 'home_team_avg_xG_last5'].values\n",
    "test_df['away_team_avg_xG_last5'] = df.loc[test_df.index, 'away_team_avg_xG_last5'].values\n",
    "\n",
    "# Drop any matches in each set where trailing xG data is not available for either team\n",
    "train_df = train_df.dropna(subset=['home_team_avg_xG_last5', 'away_team_avg_xG_last5'])\n",
    "val_df   = val_df.dropna(subset=['home_team_avg_xG_last5', 'away_team_avg_xG_last5'])\n",
    "test_df  = test_df.dropna(subset=['home_team_avg_xG_last5', 'away_team_avg_xG_last5'])\n",
    "\n",
    "# Confirm how many matches remain after dropping invalid rows\n",
    "print(\"Training set (after drop) matches:\", len(train_df))\n",
    "print(\"Validation set (after drop) matches:\", len(val_df))\n",
    "print(\"Test set (after drop) matches:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a09bf-f088-4021-8313-6f400fdbd9f8",
   "metadata": {},
   "source": [
    "__Explanation__: We combined home and away data to compute each team's rolling 5-match average xG:\n",
    "\n",
    "- We sorted combined_stats by team and date, then used groupby('Team') with a rolling window of 5. The shift(1) ensures that for a given match, we only average the previous 5 matches for that team. If a team has fewer than 5 prior matches in our dataset, the result remains NaN for those early games.\n",
    "\n",
    "- We then filled the df DataFrame's new columns using the MatchID (which corresponds to the original match index) and whether that entry was a home or away record.\n",
    "\n",
    "- After adding the new columns to df, we propagated them to our train_df, val_df, and test_df. We then dropped any rows where either home_team_avg_xG_last5 or away_team_avg_xG_last5 is NaN. This removes matches where one or both teams did not have 5 previous matches to compute an average (for example, matches in the first few weeks of a season or involving newly promoted teams without prior data).\n",
    "\n",
    "- After dropping these, the training set size will shrink slightly (losing early-season games from 2015–2016 and 2016–2017 where xG history wasn't available for some teams), and the validation and test sets may lose a few matches (typically the first few matches of the season for teams with no prior Premier League data). We now have clean splits with the required features available for all remaining matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266d719-404c-42d3-ba7f-3a2df6b1f677",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Select Features and Define the Target Variable\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc2fa1-ce77-44b1-83dc-2924407d8129",
   "metadata": {},
   "source": [
    "Now we prepare the feature matrix X and target vector y for modeling. Per the instructions, we will use the following features:\n",
    "\n",
    "- ELO_Difference – the difference in Elo rating between the home team and away team (home minus away).\n",
    "\n",
    "- home_team_avg_xG_last5 – the home team’s trailing 5-match average xG.\n",
    "\n",
    "- away_team_avg_xG_last5 – the away team’s trailing 5-match average xG.\n",
    "\n",
    "The target we predict will be whether the home team won the match. We'll create a binary target:\n",
    "\n",
    "- y = 1 if the home team won (home win),\n",
    "\n",
    "- y = 0 if the home team did not win (draw or away win).\n",
    "\n",
    "Let's construct X and y for each of the train, validation, and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628e4362-3c29-4efe-b42c-dc99193a559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home wins in training set: 0.4495697074010327\n"
     ]
    }
   ],
   "source": [
    "# Define the feature columns\n",
    "features = ['ELO_Difference', 'home_team_avg_xG_last5', 'away_team_avg_xG_last5']\n",
    "\n",
    "# Construct feature matrices for each subset\n",
    "X_train = train_df[features].values\n",
    "X_val   = val_df[features].values\n",
    "X_test  = test_df[features].values\n",
    "\n",
    "# Define the target: 1 if home win, 0 otherwise\n",
    "y_train = (train_df['FTR'] == 'H').astype(int).values\n",
    "y_val   = (val_df['FTR'] == 'H').astype(int).values\n",
    "y_test  = (test_df['FTR'] == 'H').astype(int).values\n",
    "\n",
    "# Quick sanity check on class balance in training data\n",
    "print(\"Home wins in training set:\", y_train.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fb1fe-6446-42e8-912f-6114fef09a89",
   "metadata": {},
   "source": [
    "__Explanation__: We pull the three specified feature columns into NumPy arrays for modeling. The target FTR (Full Time Result) in the dataset is a categorical label 'H', 'D', or 'A' (home win, draw, away win). We convert this to a binary outcome where 1 indicates a home win (FTR = 'H') and 0 indicates otherwise. The quick sanity check prints the proportion of home wins in the training set (for example, ~0.45 would mean 45% of training games were home wins), giving us a sense of class distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93825368-7fa3-44e0-b082-44d1768d0739",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Standardize Features (Using Only Training Data Stats)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa18466-42bd-4903-ad4d-475002d8cbbe",
   "metadata": {},
   "source": [
    "- It is good practice to standardize features (mean-center and scale to unit variance) for logistic regression, especially when features are on different scales. We must be careful to fit the scaler only on the training data, and then apply that transformation to validation and test sets, to avoid any look-ahead bias.\n",
    "\n",
    "We'll use StandardScaler from scikit-learn for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1d3cbe3-e687-44c5-ab8c-e94749ba85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fit a scaler on the training features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the features of train, val, and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0fe620-3a71-4c31-9d0a-1f61c6707e76",
   "metadata": {},
   "source": [
    "__Explanation__: We fit the StandardScaler on X_train only. This computes the mean and standard deviation of each feature using the training data. Then we transform all sets with the same scaler. This ensures that the validation and test data are scaled using parameters derived from training data only (simulating how a model in production would handle new data). Now all features are on a comparable scale, which helps the logistic regression model converge and also allows the regularization to treat features more equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61243021-e807-4041-bbd6-474d8ff3ae72",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Train Logistic Regression Models with Various Regularization Strengths\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906b690-221f-4bd1-a42b-5461e860cfa2",
   "metadata": {},
   "source": [
    "- We will train multiple logistic regression models with different regularization strengths (inverse regularization parameter C) to find the best one. The regularization used by default in scikit-learn's LogisticRegression is L2 (ridge), and C is the inverse of the regularization strength (so a smaller C means stronger regularization, and a larger C means weaker regularization).\n",
    "\n",
    "- We will try a range of C values (e.g., 0.01, 0.1, 1, 10, 100) and evaluate each model on the validation set. We'll use the F1-score on the validation set as the metric to select the best model (per the instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0aa3377-8f15-4517-80d6-f7a016d36481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-scores for different regularization strengths:\n",
      "  C = 0.01: F1 = 0.602\n",
      "  C = 0.1: F1 = 0.618\n",
      "  C = 1: F1 = 0.621\n",
      "  C = 10: F1 = 0.621\n",
      "  C = 100: F1 = 0.621\n",
      "Best regularization C = 1, which gave validation F1 = 0.620690\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define a range of C values to try\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "best_model = None\n",
    "best_C = None\n",
    "best_val_f1 = -1.0\n",
    "\n",
    "print(\"Validation F1-scores for different regularization strengths:\")\n",
    "for C in C_values:\n",
    "    # Train logistic regression with given C (using lbfgs solver for stability)\n",
    "    model = LogisticRegression(C=C, solver='lbfgs', max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on validation set and compute F1-score\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    print(f\"  C = {C}: F1 = {val_f1:.3f}\")\n",
    "    \n",
    "    # Track the best model based on validation F1\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_C = C\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best regularization C = {best_C}, which gave validation F1 = {best_val_f1:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386de5e-74a7-4539-948f-1aa7a8a9ad5f",
   "metadata": {},
   "source": [
    "__Explanation__: We loop over the candidate C values:\n",
    "\n",
    "- For each C, we train a logistic regression model on the scaled training data.\n",
    "\n",
    "- We then predict the validation set outcomes and calculate the F1-score for the positive class (home win).\n",
    "\n",
    "- We print each result for transparency. Typically, we expect some C in the middle of the range to perform best; too high C (very little regularization) or too low C (too much regularization) might hurt performance.\n",
    "\n",
    "- We keep track of the model with the highest validation F1. After the loop, best_model holds the selected logistic regression model, and best_C is its regularization strength."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e82bd4-30d3-4922-88de-8c69dbc91ab8",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Select the Best Model Using Validation F1-Score\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91adeb92-e51c-4d74-96b7-e3d1c86b4924",
   "metadata": {},
   "source": [
    "From the output above, we identify the C that yielded the highest F1-score on the validation set. We have already captured that in best_model.\n",
    "\n",
    "Let's assume (for example) that the best C turned out to be 1.0 based on the printed results (the actual output above will show the F1 for each and the best selection). The model corresponding to C=1.0 is now considered our tuned model.\n",
    "\n",
    "We will proceed using best_model as the final chosen model. (In a real scenario, one might consider re-training a fresh model on the combination of training+validation data with this hyperparameter, but here we'll continue with the model trained on the training set for evaluation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614f536-5360-4b01-9b74-932774e165df",
   "metadata": {},
   "source": [
    "---\n",
    "## \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d6f3d-6883-46fb-82b8-5f0740cfe022",
   "metadata": {},
   "source": [
    "Now we evaluate how well this chosen model performs on the test set (the 2024–2025 season, which the model has never seen). We'll compute:\n",
    "\n",
    "- __Accuracy__ – the overall proportion of correct predictions.\n",
    "\n",
    "- __F1-score__ – the harmonic mean of precision and recall for the home-win class (this focuses on how well we predicted \"home wins\").\n",
    "\n",
    "- __Confusion Matrix__ – to see the breakdown of predictions vs actual outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28808946-83fc-4c35-9cf4-34c9b0935593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.669333\n",
      "Test F1-score: 0.583893\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "[[164  57]\n",
      " [ 67  87]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.6f}\")\n",
    "print(f\"Test F1-score: {test_f1:.6f}\")\n",
    "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820120c1-e474-498c-a32d-acdf36e0b1f4",
   "metadata": {},
   "source": [
    "__Explanation__: We use the model to predict home win (1) or not (0) for each test match. The accuracy gives a general sense of performance (e.g., an accuracy of 0.67 would mean the model got 67% of matches correct overall). The F1-score specifically for home wins tells us how well the model balances precision and recall for predicting a home victory – this is useful if the classes are imbalanced (there are typically slightly more non-home-wins than home wins, since draws and away wins combined outnumber home wins slightly).\n",
    "\n",
    "The confusion matrix output is a 2x2 array:\n",
    "\n",
    "- [0,0]: True negatives (actual was 0 = not a home win, model predicted 0).\n",
    "\n",
    "- [0,1]: False positives (actual was not a home win, model predicted home win).\n",
    "\n",
    "- [1,0]: False negatives (actual was home win, model predicted not a home win).\n",
    "\n",
    "- [1,1]: True positives (actual home win, model predicted home win).\n",
    "\n",
    "For example, a confusion matrix of [[TN, FP], [FN, TP]] = [[164, 57], [67, 87]] would mean:\n",
    "\n",
    "- 164 matches where it was not a home win and the model correctly predicted that.\n",
    "\n",
    "- 57 matches where it was not a home win but the model incorrectly predicted a home win.\n",
    "\n",
    "- 67 matches where the home team did win but the model missed it.\n",
    "\n",
    "- 87 matches where the home team won and the model correctly predicted a win.\n",
    "\n",
    "From these, we could derive that in this scenario the accuracy is $(164+87) / (164+57+67+87)$ and the F1-score corresponds to the precision and recall for the 87 true positives out of predicted and actual positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265b1fa-c7f8-4c27-a1a4-38204746c8d5",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Interpret the Model Coefficients\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b4322-28e3-45af-ae8b-c2724975aad1",
   "metadata": {},
   "source": [
    "Finally, let's interpret what the logistic regression has learned from the data. We will examine the coefficients for each feature in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "397062b3-46ee-4f8c-9df7-5b5f0892d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELO_Difference: 0.755\n",
      "home_team_avg_xG_last5: 0.080\n",
      "away_team_avg_xG_last5: -0.159\n",
      "Intercept: -0.238\n"
     ]
    }
   ],
   "source": [
    "# Extract the coefficients and intercept from the best model\n",
    "feature_names = features  # ['ELO_Difference', 'home_team_avg_xG_last5', 'away_team_avg_xG_last5']\n",
    "coefs = best_model.coef_[0]    # coefficients for our three features\n",
    "intercept = best_model.intercept_[0]\n",
    "\n",
    "# Display the coefficients alongside feature names\n",
    "for name, coef in zip(feature_names, coefs):\n",
    "    print(f\"{name}: {coef:.3f}\")\n",
    "print(f\"Intercept: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda6a92-be50-4266-b96a-c3efe6fb0512",
   "metadata": {},
   "source": [
    "For correlation coefficients:\n",
    "- __+1__ means perfect positive correlation between the predictor and the target variable\n",
    "- __0__ means there is no correlation between the predictor and the target variable\n",
    "- __-1__ means perfect negative correlation between the predictor and the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef629349-19f2-46cc-9069-4cd4199f0c6c",
   "metadata": {},
   "source": [
    "__Explanation__: The logistic regression coefficients (learned on standardized features) tell us the direction and relative importance of each predictor:\n",
    "\n",
    "- __ELO_Difference__: This coefficient is typically positive and by far the largest. A positive coefficient means that as the home team's Elo rating advantage increases (home Elo minus away Elo is larger), the log-odds of a home win increase. In other words, if the home team is stronger than the away team (higher Elo), the model strongly favors a home win. For example, if this coefficient is around +0.75 (after standardization), it has a substantial impact, indicating Elo difference is a very influential predictor.\n",
    "\n",
    "- __home_team_avg_xG_last5__: This coefficient is usually positive (though likely smaller in magnitude than Elo). A positive coefficient here means if the home team has been performing well offensively in recent matches (higher average xG in last 5 games), it increases the chances of a home win. In our results, this coefficient might be relatively small (e.g., around +0.08 in standardized terms), suggesting a modest effect: recent home team form matters, but not as much as Elo.\n",
    "\n",
    "- __away_team_avg_xG_last5__: This coefficient is typically negative. A negative value means that if the away team has a high recent average xG (i.e., the away team has been in good attacking form), it decreases the likelihood of a home win (which makes sense – a strong away team performance history implies the away team could do well, thus reducing the home team's win probability). For instance, a coefficient around -0.16 (standardized) would indicate that an increase of one standard deviation in the away team's trailing xG average lowers the log-odds of a home win by 0.16. This effect is present but again smaller than the Elo effect.\n",
    "\n",
    "The __intercept__ term (e.g., around -0.24) represents the baseline log-odds of a home win when all features are at their mean values (since we standardized features, mean is 0). A slightly negative intercept in this case might correspond to the fact that, all else equal, the probability of home win is a bit less than 50% (which aligns with real-world data where home wins are somewhat less frequent than the combined probability of away win or draw).\n",
    "\n",
    "In summary, __Elo rating difference__ is the strongest predictor in our model of home wins, which aligns with domain expectations (team strength is crucial). The trailing 5-match xG averages provide additional context: a home team in good attacking form and/or an away team in poor form can tilt the odds towards a home win, whereas a high-performing away team can tilt it away. All coefficients align with intuition: stronger home team and better recent home form increase win likelihood, while strong away team recent form decreases it. This interpretation gives us confidence that the model is capturing meaningful patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaitruong",
   "language": "python",
   "name": "kaitruong"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
